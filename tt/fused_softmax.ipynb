{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Fused Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement a fused softmax using Triton, as we want to rely on Python syntax to write an efficient kernel. When dealing with operations that we want to parallelize, we first need to gain insight from the actual equation to determine which parts can be done in parallel and identify important properties that can facilitate computation.\n",
    "\n",
    "To wrap up:\n",
    "\n",
    "- Triton as a more accessible GPU programming language.\n",
    "- Analyzing the equation we want to parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Softmax Equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Eager Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In eager mode (the default in PyTorch), every line executes immediately , and the computation graph is constructed dynamically as we run the code.\n",
    "import torch\n",
    "def softmax_eq(x: torch.Tensor) -> torch.Tensor: \n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1, keepdim=True) # NOTE: In PyTorch, dim=1 means we are aggregating across the columns ~= In Triton, program_id(1) is used to access or index the column dimension.\n",
    "                                                                       # With keepdim=True (This prevent Broadcasting Errors when performing element-wise operations), the reduced dimension is retained in the output as a dimension of size 1. By default (keepdim=False), the reduction operation removes the dimension over which it is computed, resulting in a collapsed output shape. For example, summing a tensor of shape (rows, columns) along dim=1 yields a shape of (rows, 1) instead of (rows,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire computation is first recorded as a graph, then compiled and run as a fixed, optimized program.\n",
    "# Allows for optimizations and potentially faster execution once the graph is built.\n",
    "torch.compile()\n",
    "def softmax_eq_graph(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eager VS Graph Mode Bechmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager mode time per iteration: 0.000029 seconds\n",
      "Graph mode time per iteration: 0.000028 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Create a large input tensor and move it to the GPU.\n",
    "x = torch.randn(1024, 1024, device='cuda')\n",
    "\n",
    "# Warm up both functions to account for initial overhead.\n",
    "warmups = 10\n",
    "for _ in range(warmups):\n",
    "    _ = softmax_eq(x)\n",
    "    _ = softmax_eq_graph(x)\n",
    "    torch.cuda.synchronize()  # ensure GPU operations finish\n",
    "\n",
    "# Set the number of iterations for benchmarking.\n",
    "iters = 100\n",
    "\n",
    "# Benchmark Eager Mode\n",
    "start_time = time.time()\n",
    "for _ in range(iters):\n",
    "    y_eager = softmax_eq(x)\n",
    "    torch.cuda.synchronize()  # Wait for GPU operations to finish\n",
    "end_time = time.time()\n",
    "time_eager = (end_time - start_time) / iters\n",
    "\n",
    "# Benchmark Graph Mode\n",
    "start_time = time.time()\n",
    "for _ in range(iters):\n",
    "    y_graph = softmax_eq_graph(x)\n",
    "    torch.cuda.synchronize()  # Wait for GPU operations to finish\n",
    "end_time = time.time()\n",
    "time_graph = (end_time - start_time) / iters\n",
    "\n",
    "print(f\"Eager mode time per iteration: {time_eager:.6f} seconds\")\n",
    "print(f\"Graph mode time per iteration: {time_graph:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to validate our implementation of the softmax function with respect to the built-in softmax function in PyTorch. We can use the following code to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual implementation:\n",
      "tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "Torch implementation:\n",
      "tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax_pytorch = nn.Softmax(dim=1) # Requires a floating point tensor as input\n",
    "\n",
    "tiny_tensor = torch.tensor([[1e-10, 2e-10, 3e-10],\n",
    "                             [4e-10, 5e-10, 6e-10],\n",
    "                             [7e-10, 8e-10, 9e-10]], dtype=torch.float) # Casting to float to avoid overflow\n",
    "\n",
    "print(f\"Manual implementation:\\n{softmax_eq(tiny_tensor )}\")\n",
    "print(f\"Torch implementation:\\n{softmax_pytorch(tiny_tensor )}\") \n",
    "torch.allclose(softmax_pytorch(tiny_tensor), softmax_eq(tiny_tensor), atol=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our manual implementation matches the PyTorch implementation, but we used a trick by using a tiny tensor to avoid overflow. Let's try it with a larger tensor to see if our implementation is truly correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual implementation:\n",
      "tensor([[   nan,    nan,    nan],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "Torch implementation:\n",
      "tensor([[0.0000, 0.0000, 1.0000],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax_pytorch = nn.Softmax(dim=1) # Requires a floating point tensor as input\n",
    "\n",
    "input_tensor = torch.tensor([[1e10, 2e10, 3e10],\n",
    "                       [4, 5, 6],\n",
    "                       [7e-10, 8e-10, 9e-10]], dtype=torch.float) # Casting to float to avoid overflow\n",
    "\n",
    "print(f\"Manual implementation:\\n{softmax_eq(input_tensor)}\")\n",
    "print(f\"Torch implementation:\\n{softmax_pytorch(input_tensor)}\") \n",
    "print(torch.allclose(softmax_pytorch(input_tensor), softmax_eq(input_tensor), atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match PyTorch’s implementation, we should be aware that exponentiating a real number can produce large values, which may lead to overflow.\n",
    "\n",
    "To prevent this, we can subtract the maximum value from the input tensor before applying the exponential function. This does not change the result of the softmax function since the exponential function is monotonically increasing (adding or subtracting a constant from every element in the input vector doesn’t change the relative order of the values).\n",
    "$$\\operatorname{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}; \\space\n",
    "\n",
    "\\operatorname{softmax}(x_i - c) = \\frac{e^{x_i - c}}{\\sum_j e^{x_j - c}}; \\space\n",
    "\n",
    "\\operatorname{softmax}(x_i - c) = \\frac{e^{-c}e^{x_i}}{e^{-c}\\sum_j e^{x_j}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "Subtracting the maximum value from an input vector shifts all values into the negative range, ensuring that when exponentiated, they remain positive and fall within the range (0,1]. For example, e^0 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def softmax_eq_stable(x: torch.tensor) -> torch.tensor:\n",
    "    max_row_values = torch.max(x, dim=1, keepdim=True).values\n",
    "    return torch.exp(x-max_row_values) / torch.sum(torch.exp(x-max_row_values), dim=1, keepdim=True) # NOTE: In PyTorch, dim=1 means we are aggregating across the columns ~= In Triton, program_id(1) is used to access or index the column dimension.\n",
    "                                                                                                     # With keepdim=True (This prevent Broadcasting Errors when performing element-wise operations), the reduced dimension is retained in the output as a dimension of size 1. By default (keepdim=False), the reduction operation removes the dimension over which it is computed, resulting in a collapsed output shape. For example, summing a tensor of shape (rows, columns) along dim=1 yields a shape of (rows, 1) instead of (rows,). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual implementation:\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "Torch implementation:\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax_pytorch = nn.Softmax(dim=1) # Requires a floating point tensor as input\n",
    "\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7e-10, 8e-10, 9e-10]], dtype=torch.float) # Casting to float to avoid overflow\n",
    "\n",
    "print(f\"Manual implementation:\\n{softmax_eq_stable(input_tensor)}\")\n",
    "print(f\"Torch implementation:\\n{softmax_pytorch(input_tensor)}\") \n",
    "print(torch.allclose(softmax_pytorch(input_tensor), softmax_eq_stable(input_tensor), atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have gained insight into how to implement the softmax equation and its corresponding stable version, which leverages the monotonically increasing property of the function, we can inspect Triton’s tutorial on the naive softmax implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908')\n",
    "\n",
    "\n",
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0] # == x.max(dim=1).values. Shape: [rows] Column vector\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None] # Reshapes x_max from shape (M,) to (M, 1) for proper broadcasting with x.\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z) # Shape: [M,N]\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1) # Shape: [rows]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None] # Reshapes denominator from shape (M,) to (M, 1) for proper broadcasting with numerator. [M,N]/[M,None] = [M,i]/[M,1] for i in range(0,N)\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties: {'max_shared_mem': 232448, 'max_num_regs': 65536, 'multiprocessor_count': 132, 'warpSize': 32, 'sm_clock_rate': 1980000, 'mem_clock_rate': 2619000, 'mem_bus_width': 5120}\n",
      "Triton output:\n",
      "tensor([[3.2747e-04, 5.3954e-04, 5.8687e-05,  ..., 5.5490e-04, 2.0330e-04,\n",
      "         9.4091e-04],\n",
      "        [8.6914e-04, 1.1075e-03, 1.0202e-04,  ..., 1.1470e-04, 5.5436e-04,\n",
      "         8.6046e-04],\n",
      "        [1.4101e-03, 1.0511e-03, 1.7031e-03,  ..., 1.2272e-03, 7.9059e-04,\n",
      "         3.2380e-04],\n",
      "        ...,\n",
      "        [3.3964e-03, 1.8625e-03, 8.0877e-04,  ..., 1.4277e-03, 1.0067e-03,\n",
      "         3.4911e-04],\n",
      "        [6.2853e-03, 2.9364e-03, 4.4708e-04,  ..., 5.5559e-04, 6.0394e-03,\n",
      "         4.5373e-04],\n",
      "        [3.3917e-03, 6.1801e-04, 2.0096e-03,  ..., 9.9985e-03, 4.4419e-04,\n",
      "         9.4954e-04]], device='cuda:0')\n",
      "Torch output:\n",
      "tensor([[3.2747e-04, 5.3954e-04, 5.8687e-05,  ..., 5.5490e-04, 2.0330e-04,\n",
      "         9.4091e-04],\n",
      "        [8.6914e-04, 1.1075e-03, 1.0202e-04,  ..., 1.1470e-04, 5.5436e-04,\n",
      "         8.6046e-04],\n",
      "        [1.4101e-03, 1.0511e-03, 1.7031e-03,  ..., 1.2272e-03, 7.9059e-04,\n",
      "         3.2380e-04],\n",
      "        ...,\n",
      "        [3.3964e-03, 1.8625e-03, 8.0877e-04,  ..., 1.4277e-03, 1.0067e-03,\n",
      "         3.4911e-04],\n",
      "        [6.2853e-03, 2.9364e-03, 4.4708e-04,  ..., 5.5559e-04, 6.0394e-03,\n",
      "         4.5373e-04],\n",
      "        [3.3917e-03, 6.1801e-04, 2.0096e-03,  ..., 9.9985e-03, 4.4419e-04,\n",
      "         9.4954e-04]], device='cuda:0')\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0) # PyTorch Eager Implementation: The tensor is 2D, so we sum along dim=1 (columns). \n",
    "                                                  # Triton Kernel Implementation: Each program processes one row (a 1D tensor), so we sum along axis 0 (the only axis).\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0) # PyTorch Implementation: The tensor is 2D, so we sum along dim=1 (columns). \n",
    "                                                # Triton Kernel Implementation: Each program processes one row (a 1D tensor), so we sum along axis 0 (the only axis).\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "# ---- #\n",
    "# We can create a wrapper function that enqueues the kernel along with its associated (meta-)arguments for any given input tensor.\n",
    "# ---- #\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "print(f\"Properties: {properties}\")\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols) # To fit an entire row into a single block. This allows us to fit all rows in the GPU’s SRAM, maximizing memory bandwidth and minimizing memory latency.\n",
    "\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "        # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "        # ISA SECTION (3.6.4 for CDNA3)\n",
    "        # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "        # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "        # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "        # not required to be equal numbers of both types.\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "        # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "        # execute on a CU (multi-processor)  in parallel.\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols) # We are launching multiple programs in parallel, each processing one row of the input tensor.\n",
    "                                                                                 # We don't need to pass tl.constexpr variable twice, as they are compile-time constants.\n",
    "    return y\n",
    "\n",
    "# ---- #\n",
    "# Unit Test: \n",
    "# A matrix input with irregular numbers of rows and columns -- TO VERIFY --> Padding mechanism works.\n",
    "# ---- #\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "y_triton = softmax(x)\n",
    "print(f\"Triton output:\\n{y_triton}\")\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "print(f\"Torch output:\\n{y_torch}\")\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters marked with `tl.constexpr` are treated as compile‐time constants. Their values are \"baked in\" during compilation, so we don’t need (and shouldn’t) pass them again at runtime.\n",
    "\n",
    "It is worth noting that after researching the [Triton interpreter code](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/interpreter.py#L1210-L1240), we found that the `GridExecutor` class handles this by filtering out extra keyword arguments, such as those marked as `tl.constexpr`. In its constructor, it records the names of parameters annotated as \"constexpr\", and in its `__call__` method, it filters the keyword arguments so that only those matching the function's declared parameters are passed. This mechanism prevents the compile‐time (constexpr) parameters from being re-passed at runtime, avoiding argument count mismatches.\n",
    "\n",
    "Nonetheless, if we’re using an outdated version of the interpreter, this filtering might not work as expected. As a best practice, we should avoid passing these constexpr parameters again at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a snippet of code from the Triton Interpreter codebase.\n",
    "# It is intended to show the implementation and is not executable as-is.\n",
    "class GridExecutor:\n",
    "\n",
    "    def __init__(self, fn, arg_names, grid):\n",
    "        from .jit import _normalize_ty  # TODO: modularize\n",
    "\n",
    "        self.fn = fn\n",
    "        self.arg_names = arg_names\n",
    "        self.grid = grid\n",
    "        __annotations__ = {name: _normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n",
    "        self.constexprs = [name for name in arg_names if __annotations__.get(name) == \"constexpr\"]\n",
    "\n",
    "    # ---- #\n",
    "    # Scaping few lines of code for brevity\n",
    "    # ---- #\n",
    "    \n",
    "    def __call__(self, *args_dev, **kwargs):\n",
    "            if kwargs.pop(\"warmup\", False):\n",
    "                return\n",
    "            # Removes not used reserved keywords from kwargs\n",
    "            # Triton doesn't support keyword-only, variable positional or variable keyword arguments\n",
    "            # It's safe to inspect only positional or keyword arguments (i.e., argspec.args)\n",
    "            argspec = inspect.getfullargspec(self.fn)\n",
    "            kwargs = {k: v for k, v in kwargs.items() if k in argspec.args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triton Reserved Keywords in our Kernel: [check-triton-lang-python](https://github.com/triton-lang/triton/tree/f73cf3268ef04d862493e0fc1cca5257f2a09346/python/triton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Triton helps ease our minds with its excellent interpreter mode, which overcomes the challenges of GPU debugging by simulating kernel execution sequentially on the CPU, let's use this feature to gain a deeper understanding of what happens in the fused implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRITON_INTERPRETER=1\n",
      "Properties: {'max_shared_mem': 232448, 'max_num_regs': 65536, 'multiprocessor_count': 132, 'warpSize': 32, 'sm_clock_rate': 1980000, 'mem_clock_rate': 2619000, 'mem_bus_width': 5120}\n",
      "n_cols:  3\n",
      "Greater power of two greater than the numbers of columns in 'x': BLOCK_SIZE:  4\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (0, 0, 0) idx () row_start (pid): 0\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_start (pid): 1\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (0, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "pid (1, 0, 0) idx () row_step: 2\n",
      "Triton output:\n",
      "tensor([[0.3538, 0.5828, 0.0634],\n",
      "        [0.4443, 0.3405, 0.2152]], device='cuda:0')\n",
      "Torch output:\n",
      "tensor([[0.3538, 0.5828, 0.0634],\n",
      "        [0.4443, 0.3405, 0.2152]], device='cuda:0')\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "%env TRITON_INTERPRETER=1\n",
    "# Operator: This is the GPU kernel written in Triton that performs the actual Softmax computation.\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    print(\"row_start (pid): \", row_start)\n",
    "    # print(\"row_start: \", row_start)\n",
    "    row_step = tl.num_programs(0)\n",
    "    print(\"row_step: \", row_step)\n",
    "    # print(\"row_step: \", row_step)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols # Indicates which addresses are valid.\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf')) # other=-float('inf') For invalid addresses, fill with negative infinity to ensure it does not affect the result when calculating the maximum value in subsequent computations.\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0) # PyTorch Implementation: The tensor is 2D, so we sum along dim=1 (columns). \n",
    "                                                  # Triton Kernel Implementation: Each program processes one row (a 1D tensor), so we sum along axis 0 (the only axis).\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0) # PyTorch Implementation: The tensor is 2D, so we sum along dim=1 (columns). \n",
    "                                                # Triton Kernel Implementation: Each program processes one row (a 1D tensor), so we sum along axis 0 (the only axis).\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride # The starting address of the current row in the output tensor.\n",
    "        output_ptrs = output_row_start_ptr + col_offsets # The address of each element in the current row of the output tensor.\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask) # Writes the results back to memory, using the same mask as loading to ensure only valid data is written back.\n",
    "\n",
    "# ---- #\n",
    "# We can create a wrapper function that enqueues the kernel along with its associated (meta-)arguments for any given input tensor.\n",
    "# ---- #\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "print(f\"Properties: {properties}\")\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "# Driver program that sets a lot of meta information, such as block size, shared memory allocation, etc. A top-down approach is used to determine the optimal configuration.\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols) # Each kernel instance (each block program) can load an entire row of the input tensor into shared memory.\n",
    "                                                # We have used a grid of 1 dimension, so we are using dim=0 (row dimension). Then, the width of the 2D matrix dimension (n_cols) is used to calculate the next power of 2.\n",
    "                                                # Example: input = 2x3 matrix, n_cols = 3, next_power_of_2(3) = 4 -> 2^0 = 1; 2^1 = 2; 2^2 = 4 > 3; 2^3 = 8 > 3. So, BLOCK_SIZE = 4  \n",
    "    print(\"n_cols: \", n_cols)\n",
    "    print(\"Greater power of two greater than the numbers of columns in 'x': BLOCK_SIZE: \", BLOCK_SIZE)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                  num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "        # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "        # ISA SECTION (3.6.4 for CDNA3)\n",
    "        # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "        # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "        # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "        # not required to be equal numbers of both types.\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "        # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "        # execute on a CU (multi-processor)  in parallel.\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols) # Since our programs are defined along the row dimension (dim=0), we need to compute the number of elements per row (i.e., the row stride) to properly space the memory accesses.\n",
    "                                                                                 # We don't need to pass tl.constexpr variable twice, as they are compile-time constants. \n",
    "    return y\n",
    "\n",
    "# ---- #\n",
    "# Unit Test: \n",
    "# A matrix input with irregular numbers of rows and columns -- TO VERIFY --> Padding mechanism works.\n",
    "# ---- #\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(2, 3, device=DEVICE)\n",
    "y_triton = softmax(x)\n",
    "print(f\"Triton output:\\n{y_triton}\")\n",
    "y_torch = torch.softmax(x, axis=1) # Apply softmax along axis 1 (aggregate over columns, resulting in row-wise normalization)\n",
    "print(f\"Torch output:\\n{y_torch}\")\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of Triton is that each block must contain a power-of-two number of elements. This requirement eliminates the need for extra boundary checks that would be necessary with uneven divisions. However, since input data may not naturally align to a power-of-two size, we pad the data to the nearest power-of-two and track the valid memory addresses, masking the padded elements during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      "tensor([[-0.9247, -0.4253, -2.6438],\n",
      "        [ 0.1452, -0.1209, -0.5797]], device='cuda:0')\n",
      "Row offsets (indices) (pid): tensor([0, 1])\n",
      "Column offsets (indices) (power of two): tensor([0, 1, 2, 3]) \n",
      "\n",
      " row\\col offsets    col   0   col   1   col   2   col   3\n",
      "           row 0   -0.9247   -0.4253   -2.6438\n",
      "           row 1    0.1452   -0.1209   -0.5797\n",
      "Number of programs = Number of rows as we are using a grid of 1 dimension (tl.program_id(0)): 2 \n",
      "\n",
      " row\\col offsets    col   0   col   1   col   2   col   3\n",
      " row(pid)( 0,0,0)   -0.9247   -0.4253   -2.6438\n",
      " row(pid)( 1,0,0)    0.1452   -0.1209   -0.5797\n",
      "\n",
      " Masking to guard against out-of-bounds accesses\n",
      " row\\col offsets    col   0   col   1   col   2   col   3\n",
      " row(pid)( 0,0,0)        -1        -1        -1         0\n",
      " row(pid)( 1,0,0)        -1        -1        -1         0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(2, 3, device=DEVICE)\n",
    "print(f\"Input tensor:\\n{x}\")\n",
    "\n",
    "n_cols = x.size(1)\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "# Generate row and column offsets (indices)\n",
    "row_offsets = torch.arange(x.size(0))\n",
    "col_offsets = torch.arange(BLOCK_SIZE)\n",
    "\n",
    "print(\"Row offsets (indices) (pid):\", row_offsets)\n",
    "print(\"Column offsets (indices) (power of two):\", col_offsets, \"\\n\")\n",
    "# Print header with column offsets\n",
    "header = \" row\\col offsets    \" + \"   \".join([f\"col{int(c.item()):>4}\" for c in col_offsets])\n",
    "print(f\"{header}\")\n",
    "\n",
    "# For each row, print the row offset followed by the row values.\n",
    "for i in range(x.size(0)):\n",
    "    row_str = f\"           row{int(row_offsets[i].item()):>2}  \" + \"  \".join([f\"{x[i, j].item():>8.4f}\" for j in range(x.size(1))])\n",
    "    print(row_str)\n",
    "\n",
    "print(f\"Number of programs = Number of rows as we are using a grid of 1 dimension (tl.program_id(0)): {x.size(0)} \\n\")\n",
    "\n",
    "header = \" row\\col offsets    \" + \"   \".join([f\"col{int(c.item()):>4}\" for c in col_offsets])\n",
    "print(header)\n",
    "\n",
    "# For each row, print the row offset followed by the row values.\n",
    "for i in range(x.size(0)):\n",
    "    row_str = f\" row(pid)({int(row_offsets[i].item()):>2},0,0)  \" + \"  \".join([f\"{x[i, j].item():>8.4f}\" for j in range(x.size(1))])\n",
    "    print(row_str)\n",
    "\n",
    "# Masking to guard against out-of-bounds accesses\n",
    "print(\"\\n Masking to guard against out-of-bounds accesses\")\n",
    "header = \" row\\col offsets    \" + \"   \".join([f\"col{int(c.item()):>4}\" for c in col_offsets])\n",
    "print(header)\n",
    "for i in range(x.size(0)):\n",
    "    row_str = f\" row(pid)({int(row_offsets[i].item()):>2},0,0)        \" + \"        \".join([\"-1\" if j < n_cols else \" 0\" for j in range(col_offsets.size(0))])\n",
    "    print(row_str)\n",
    "\n",
    "mask = col_offsets < n_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For profiling, we can use Nsight Compute to profile our kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 615655 (/home/alex/miniforge3/envs/triton/bin/python3.10)\n",
      "Properties: {'max_shared_mem': 232448, 'max_num_regs': 65536, 'multiprocessor_count': 132, 'warpSize': 32, 'sm_clock_rate': 1980000, 'mem_clock_rate': 2619000, 'mem_bus_width': 5120}\n",
      "==PROF== Profiling \"distribution_elementwise_grid...\" - 0: 0%....50%....100% - 10 passes\n",
      "==PROF== Profiling \"softmax_kernel\" - 1: 0%....50%....100% - 10 passes\n",
      "==PROF== Profiling \"softmax_warp_forward\" - 2: 0%....50%....100% - 10 passes\n",
      "==PROF== Disconnected from process 615655\n",
      "[615655] python3.10@127.0.0.1\n",
      "  void at::native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, (int)4, void at::native::templates::cuda::normal_and_transform<float, float, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_and_transform<float, float, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ------------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ----------------------- ------------- ------------\n",
      "    DRAM Frequency          cycle/nsecond         2.60\n",
      "    SM Frequency            cycle/nsecond         1.60\n",
      "    Elapsed Cycles                  cycle        5,625\n",
      "    Memory Throughput                   %         8.98\n",
      "    DRAM Throughput                     %         0.09\n",
      "    Duration                      usecond         3.52\n",
      "    L1/TEX Cache Throughput             %        43.06\n",
      "    L2 Cache Throughput                 %        10.21\n",
      "    SM Active Cycles                cycle        16.26\n",
      "    Compute (SM) Throughput             %         0.09\n",
      "    ----------------------- ------------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Cluster Scheduling Policy                           PolicySpread\n",
      "    Cluster Size                                                   0\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              43\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM             132\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.00\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 99.24%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Max Active Clusters                 cluster            0\n",
      "    Max Cluster Size                      block            8\n",
      "    Overall GPU Occupancy                     %            0\n",
      "    Cluster Occupancy                         %            0\n",
      "    Block Limit SM                        block           32\n",
      "    Block Limit Registers                 block            5\n",
      "    Block Limit Shared Mem                block           32\n",
      "    Block Limit Warps                     block            8\n",
      "    Theoretical Active Warps per SM        warp           40\n",
      "    Theoretical Occupancy                     %        62.50\n",
      "    Achieved Occupancy                        %        12.44\n",
      "    Achieved Active Warps Per SM           warp         7.96\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 80.09%                                                                                    \n",
      "          The difference between calculated theoretical (62.5%) and measured achieved occupancy (12.4%) can be the      \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Local Speedup: 37.5%                                                                                     \n",
      "          The 10.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      \n",
      "          hardware maximum of 16. This kernel's theoretical occupancy (62.5%) is limited by the number of required      \n",
      "          registers.                                                                                                    \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle         7.80\n",
      "    Total DRAM Elapsed Cycles        cycle      366,592\n",
      "    Average L1 Active Cycles         cycle        16.26\n",
      "    Total L1 Elapsed Cycles          cycle      742,108\n",
      "    Average L2 Active Cycles         cycle     1,082.51\n",
      "    Total L2 Elapsed Cycles          cycle      478,320\n",
      "    Average SM Active Cycles         cycle        16.26\n",
      "    Total SM Elapsed Cycles          cycle      742,108\n",
      "    Average SMSP Active Cycles       cycle        16.16\n",
      "    Total SMSP Elapsed Cycles        cycle    2,968,432\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "  softmax_kernel (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ------------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ----------------------- ------------- ------------\n",
      "    DRAM Frequency          cycle/nsecond         2.62\n",
      "    SM Frequency            cycle/nsecond         1.60\n",
      "    Elapsed Cycles                  cycle        5,428\n",
      "    Memory Throughput                   %         9.28\n",
      "    DRAM Throughput                     %         0.04\n",
      "    Duration                      usecond         3.39\n",
      "    L1/TEX Cache Throughput             %        23.61\n",
      "    L2 Cache Throughput                 %        10.66\n",
      "    SM Active Cycles                cycle        29.65\n",
      "    Compute (SM) Throughput             %         0.08\n",
      "    ----------------------- ------------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Cluster Scheduling Policy                           PolicySpread\n",
      "    Cluster Size                                                   0\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      2\n",
      "    Registers Per Thread             register/thread              28\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block              48\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM             132\n",
      "    Threads                                   thread             512\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.00\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 98.48%                                                                                          \n",
      "          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 132             \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Max Active Clusters                 cluster            0\n",
      "    Max Cluster Size                      block            8\n",
      "    Overall GPU Occupancy                     %            0\n",
      "    Cluster Occupancy                         %            0\n",
      "    Block Limit SM                        block           32\n",
      "    Block Limit Registers                 block            8\n",
      "    Block Limit Shared Mem                block           28\n",
      "    Block Limit Warps                     block            8\n",
      "    Theoretical Active Warps per SM        warp           64\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        12.44\n",
      "    Achieved Active Warps Per SM           warp         7.96\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 87.56%                                                                                    \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.4%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle         3.60\n",
      "    Total DRAM Elapsed Cycles        cycle      354,944\n",
      "    Average L1 Active Cycles         cycle        29.65\n",
      "    Total L1 Elapsed Cycles          cycle      715,960\n",
      "    Average L2 Active Cycles         cycle     1,091.75\n",
      "    Total L2 Elapsed Cycles          cycle      461,440\n",
      "    Average SM Active Cycles         cycle        29.65\n",
      "    Total SM Elapsed Cycles          cycle      715,960\n",
      "    Average SMSP Active Cycles       cycle        28.64\n",
      "    Total SMSP Elapsed Cycles        cycle    2,863,840\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 6.281%                                                                                          \n",
      "          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n",
      "          Maximum instance value is 33.19% above the average, while the minimum instance value is 11.15% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "  void <unnamed>::softmax_warp_forward<float, float, float, (int)2, (bool)0, (bool)0>(T2 *, const T1 *, int, int, int, const bool *, int, bool) (1, 1, 1)x(4, 32, 1), Context 1, Stream 7, Device 0, CC 9.0\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ------------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ----------------------- ------------- ------------\n",
      "    DRAM Frequency          cycle/nsecond         2.59\n",
      "    SM Frequency            cycle/nsecond         1.59\n",
      "    Elapsed Cycles                  cycle        5,179\n",
      "    Memory Throughput                   %         9.72\n",
      "    DRAM Throughput                     %         0.06\n",
      "    Duration                      usecond         3.26\n",
      "    L1/TEX Cache Throughput             %        50.57\n",
      "    L2 Cache Throughput                 %        11.11\n",
      "    SM Active Cycles                cycle        13.84\n",
      "    Compute (SM) Throughput             %         0.01\n",
      "    ----------------------- ------------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   128\n",
      "    Cluster Scheduling Policy                           PolicySpread\n",
      "    Cluster Size                                                   0\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              21\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM             132\n",
      "    Threads                                   thread             128\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.00\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 99.24%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Max Active Clusters                 cluster            0\n",
      "    Max Cluster Size                      block            8\n",
      "    Overall GPU Occupancy                     %            0\n",
      "    Cluster Occupancy                         %            0\n",
      "    Block Limit SM                        block           32\n",
      "    Block Limit Registers                 block           21\n",
      "    Block Limit Shared Mem                block           32\n",
      "    Block Limit Warps                     block           16\n",
      "    Theoretical Active Warps per SM        warp           64\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %         3.76\n",
      "    Achieved Active Warps Per SM           warp         2.41\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.24%                                                                                    \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (3.8%) can be the      \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle         5.20\n",
      "    Total DRAM Elapsed Cycles        cycle      337,920\n",
      "    Average L1 Active Cycles         cycle        13.84\n",
      "    Total L1 Elapsed Cycles          cycle      683,328\n",
      "    Average L2 Active Cycles         cycle     1,102.99\n",
      "    Total L2 Elapsed Cycles          cycle      440,480\n",
      "    Average SM Active Cycles         cycle        13.84\n",
      "    Total SM Elapsed Cycles          cycle      683,328\n",
      "    Average SMSP Active Cycles       cycle         8.10\n",
      "    Total SMSP Elapsed Cycles        cycle    2,733,312\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 6.057%                                                                                          \n",
      "          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n",
      "          Maximum instance value is 30.23% above the average, while the minimum instance value is 13.14% below the      \n",
      "          average.                                                                                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ncu --target-processes all $(which python) fused_softmax_nsight.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bechmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding Triton’s fused softmax implementation, we want to benchmark its performance against PyTorch JIT. To do so, we can use the built-in Triton benchmarking package by making the benchmarking depend on (a function of) the number of columns for a matrix with a fixed row dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax-performance:\n",
      "          N       Triton        Torch\n",
      "0     256.0   747.422644  1031.847758\n",
      "1     384.0  1060.653040  1245.379774\n",
      "2     512.0  1393.056979  1419.245989\n",
      "3     640.0  1524.467622  1579.269816\n",
      "4     768.0  1768.862475  1781.544042\n",
      "5     896.0  1948.950213  1880.204631\n",
      "6    1024.0  1988.966142  1933.369713\n",
      "7    1152.0  2002.839518   940.762018\n",
      "8    1280.0  2092.294574  1041.748986\n",
      "9    1408.0  2194.412535  1119.133928\n",
      "10   1536.0  2191.726775  1191.297787\n",
      "11   1664.0  2290.507272  1253.634565\n",
      "12   1792.0  2330.106884  1309.227803\n",
      "13   1920.0  2371.925007  1386.144464\n",
      "14   2048.0  2398.696114  1423.695438\n",
      "15   2176.0  2240.827929  1494.137819\n",
      "16   2304.0  2328.065694  1582.491404\n",
      "17   2432.0  2351.222882  1630.998390\n",
      "18   2560.0  2418.567543  1656.881817\n",
      "19   2688.0  2476.234896  1708.367841\n",
      "20   2816.0  2479.048065  1800.323389\n",
      "21   2944.0  2466.164869  1808.862921\n",
      "22   3072.0  2478.470616  1858.421916\n",
      "23   3200.0  2526.020791  1894.413036\n",
      "24   3328.0  2529.099199  1933.477825\n",
      "25   3456.0  2533.559182  1954.766147\n",
      "26   3584.0  2547.738781  1988.128779\n",
      "27   3712.0  2568.986156  2045.046463\n",
      "28   3840.0  2601.301691  2080.671773\n",
      "29   3968.0  2584.004806  2088.954740\n",
      "30   4096.0  2672.747130  2105.724150\n",
      "31   4224.0  2524.224871  1841.204877\n",
      "32   4352.0  2503.715411  1802.689433\n",
      "33   4480.0  2493.621068  1814.799931\n",
      "34   4608.0  2504.245576  1815.557464\n",
      "35   4736.0  2534.456515  1842.799850\n",
      "36   4864.0  2567.933359  1867.761180\n",
      "37   4992.0  2568.179210  1889.583190\n",
      "38   5120.0  2607.888468  1922.551194\n",
      "39   5248.0  2611.387932  1955.193192\n",
      "40   5376.0  2573.088371  1992.138085\n",
      "41   5504.0  2630.891162  2021.763894\n",
      "42   5632.0  2647.589397  2047.083634\n",
      "43   5760.0  2673.231844  2082.988329\n",
      "44   5888.0  2597.687559  2095.398368\n",
      "45   6016.0  2653.046844  2125.040002\n",
      "46   6144.0  2650.250902  2153.459227\n",
      "47   6272.0  2652.294305  2167.199936\n",
      "48   6400.0  2689.968324  2206.014080\n",
      "49   6528.0  2656.767436  2223.796547\n",
      "50   6656.0  2699.737323  2247.025403\n",
      "51   6784.0  2685.448947  2259.647869\n",
      "52   6912.0  2718.521895  2277.665318\n",
      "53   7040.0  2681.998637  2303.942986\n",
      "54   7168.0  2697.035948  2326.571673\n",
      "55   7296.0  2723.514423  2348.834536\n",
      "56   7424.0  2687.339525  2371.862857\n",
      "57   7552.0  2734.311423  2389.190411\n",
      "58   7680.0  2746.750301  2393.141969\n",
      "59   7808.0  2755.994229  2422.221779\n",
      "60   7936.0  2759.156982  2462.098617\n",
      "61   8064.0  2677.673094  2473.043464\n",
      "62   8192.0  2734.902992  2500.282064\n",
      "63   8320.0  2719.302662  2349.183970\n",
      "64   8448.0  2699.449529  2321.745565\n",
      "65   8576.0  2699.240793  2297.445040\n",
      "66   8704.0  2681.130878  2314.480157\n",
      "67   8832.0  2683.067349  2309.708559\n",
      "68   8960.0  2671.434011  2303.776163\n",
      "69   9088.0  2674.426203  2317.808654\n",
      "70   9216.0  2673.651169  2327.451290\n",
      "71   9344.0  2691.660677  2322.819019\n",
      "72   9472.0  2663.401270  2340.918453\n",
      "73   9600.0  2681.855257  2340.870758\n",
      "74   9728.0  2683.351684  2366.615282\n",
      "75   9856.0  2686.718722  2374.913348\n",
      "76   9984.0  2664.891312  2378.354625\n",
      "77  10112.0  2671.881394  2400.547717\n",
      "78  10240.0  2700.974589  2403.307182\n",
      "79  10368.0  2667.692400  2428.751836\n",
      "80  10496.0  2671.993002  2442.481613\n",
      "81  10624.0  2667.505093  2455.328179\n",
      "82  10752.0  2685.687763  2472.829563\n",
      "83  10880.0  2702.547926  2483.826108\n",
      "84  11008.0  2712.699586  2488.905705\n",
      "85  11136.0  2664.838511  2499.169826\n",
      "86  11264.0  2712.343987  2527.275870\n",
      "87  11392.0  2717.704509  2527.823100\n",
      "88  11520.0  2719.247674  2544.891201\n",
      "89  11648.0  2728.397776  2541.039089\n",
      "90  11776.0  2682.727498  2545.557103\n",
      "91  11904.0  2730.555297  2579.752104\n",
      "92  12032.0  2721.638182  2586.991579\n",
      "93  12160.0  2748.101366  2594.238662\n",
      "94  12288.0  2735.814674  1702.120550\n",
      "95  12416.0  2675.203172  1666.022060\n",
      "96  12544.0  2733.593288  1648.910489\n",
      "97  12672.0  2753.542413  1644.935078\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "# We instantiate the kernel without interpreter mode to properly launch it on the GPU.\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    \n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "   \n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        row_minus_max = row - tl.max(row, axis=0) \n",
    "\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0) \n",
    "        softmax_output = numerator / denominator\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "# ---- #\n",
    "# We can create a wrapper function that enqueues the kernel along with its associated (meta-)arguments for any given input tensor.\n",
    "# ---- #\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    num_warps = 8\n",
    "\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols) \n",
    "    return y\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
